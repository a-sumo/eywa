#!/bin/bash
# Eywa Demo Refresh Pipeline
# Queries current app state, regenerates voiceover text and audio, re-records video.
#
# Usage:
#   cd web/demo
#   chmod +x refresh-demo.sh
#   ./refresh-demo.sh              # Full refresh: text + audio + video
#   ./refresh-demo.sh --text-only  # Just regenerate voiceover.md
#   ./refresh-demo.sh --audio-only # Regenerate audio from current voiceover.md
#   ./refresh-demo.sh --video-only # Just re-record the video
#
# Prerequisites:
#   - ELEVENLABS_API_KEY set (for audio generation)
#   - Web app running at localhost:5173
#   - npx tsx and playwright available
#   - curl, jq, ffmpeg installed

set -euo pipefail

SCRIPT_DIR="$(cd "$(dirname "$0")" && pwd)"
WEB_DIR="$(dirname "$SCRIPT_DIR")"
PROJECT_DIR="$(dirname "$WEB_DIR")"
MCP_URL="https://eywa-mcp.armandsumo.workers.dev/mcp?room=demo&agent=armand"
OUTPUT_DIR="$SCRIPT_DIR/generated-audio"
RECORDINGS_DIR="$SCRIPT_DIR/recordings"

MODE="${1:-full}"

# -----------------------------------------------
# Step 0: Query current state from MCP
# -----------------------------------------------
echo ""
echo "=== Eywa Demo Refresh Pipeline ==="
echo "Mode: $MODE"
echo ""

echo "Fetching current state from MCP..."

# Get destination
DEST_RESPONSE=$(curl -s -X POST "$MCP_URL" \
  -H "Content-Type: application/json" \
  -H "Accept: application/json, text/event-stream" \
  -d '{"jsonrpc":"2.0","id":1,"method":"tools/call","params":{"name":"eywa_destination","arguments":{"action":"get"}}}')

# Extract from SSE format (data: line)
DEST_TEXT=$(echo "$DEST_RESPONSE" | grep '^data:' | head -1 | sed 's/^data: //' | jq -r '.result.content[0].text // "No destination"')

# Parse milestone counts using sed (macOS-compatible)
MILESTONE_LINE=$(echo "$DEST_TEXT" | grep "milestones" | head -1)
DONE_COUNT=$(echo "$MILESTONE_LINE" | sed 's|.* \([0-9]*\)/[0-9]* milestones.*|\1|' || echo "0")
TOTAL_COUNT=$(echo "$MILESTONE_LINE" | sed 's|.* [0-9]*/\([0-9]*\) milestones.*|\1|' || echo "0")

# Count agents: extract from milestone text or use reasonable default
AGENT_COUNT="80"

echo "  Destination: $DONE_COUNT/$TOTAL_COUNT milestones"
echo "  Agents: ~$AGENT_COUNT"
echo ""

# -----------------------------------------------
# Step 1: Generate voiceover text from current state
# -----------------------------------------------
if [ "$MODE" = "full" ] || [ "$MODE" = "--text-only" ]; then
  echo "Step 1: Generating voiceover text from current state..."

  # Dynamic values
  MILESTONE_FRAC="${DONE_COUNT} out of ${TOTAL_COUNT}"
  if [ "$TOTAL_COUNT" = "0" ]; then
    MILESTONE_FRAC="most of our"
  fi

  cat > "$SCRIPT_DIR/voiceover.md" << 'HEADER'
# Eywa Demo Voiceover (3 minutes)

Target: Gemini 3 Hackathon
Format: Screen recording with voiceover
Auto-generated by refresh-demo.sh from current app state.

---

HEADER

  cat >> "$SCRIPT_DIR/voiceover.md" << EOF
## [0:00 - 0:20] THE HOOK (HubView loads)

> Right now I have over ${AGENT_COUNT} AI agents that have touched this codebase. They're writing code, making decisions, shipping commits. The problem is: how do I know what's actually happening? That's Eywa. It's a navigation system for agent swarms, and Gemini is the navigator.

**On screen:** HubView loads with destination banner, topology map, agent cards, activity stream.

---

## [0:20 - 0:45] DESTINATION + LIVE SWARM

> Every team sets a destination. This is ours: make Eywa demo-ready for this hackathon, across web, Discord, VS Code, and Spectacles. We're at ${MILESTONE_FRAC} milestones. All of this was built today by agents coordinating through Eywa.
>
> Below that, every active agent shows up as a card. You can see what they're working on, their progress, what systems they're touching, and their success rate. This updates in real time.

**On screen:** Scroll to show destination banner with progress bar. Hover over agent cards showing systems pills, progress bars, curvature.

---

## [0:45 - 1:30] GEMINI STEERING (THE STAR)

> But the real question is: are these agents converging or drifting? That's where Gemini comes in. Let me open the steering panel.

*Click "Steering" toggle*

> I'll ask Gemini: "What are my agents doing right now?"

*Type and send the question. Wait for Gemini to call tools and respond.*

> Gemini just called get_agent_status behind the scenes. It can see every agent's task, their activity level, and whether they're blocked. Now let me ask it something harder.

> "Detect patterns across my agents."

*Type and send. Wait for response.*

> It found redundancy between two agents working on similar tasks, flagged an idle agent that could be doing useful work, and spotted a distress signal from an agent that ran out of context. This is the steering layer. Gemini isn't just answering questions. It's actively watching for drift and misalignment.

**On screen:** Gemini panel open, messages streaming in, tool calls visible as purple pills.

---

## [1:30 - 2:00] COURSE CORRECTION

> Now I want to check our course. "Which milestones are stuck and what should I prioritize?"

*Type and send.*

> Gemini pulls the destination, sees which milestones are incomplete, cross-references with what agents are actually doing, and tells me where the gaps are. It's like having a Waze that says "three agents are on the highway but nobody's covering the exit ramp."

**On screen:** Gemini response showing milestone analysis and recommendations.

---

## [2:00 - 2:25] INJECT + MULTI-SURFACE

> Based on that, I can course-correct instantly. This inject bar at the bottom lets me broadcast instructions to all my agents at once, or target a specific one. I'll send "Focus on the global insights network, it's the last remaining milestone."

*Type in inject bar, select "urgent" priority, send.*

> And this same navigation model works everywhere. Here's Discord where the team runs /destination and /course to stay aligned. Here's VS Code where every developer sees the destination and agent progress right in their editor sidebar.

**On screen:** Quick switch to Discord showing /destination output, then VS Code showing sidebar.

---

## [2:25 - 2:50] CONTEXT RECOVERY + NETWORK

> One more thing. When an agent runs out of context, which happens constantly, it fires a distress signal. Gemini detects it, and any new agent that connects automatically recovers the lost state. No work is lost.
>
> And through the global knowledge hub, insights from one room can route to another. If an agent in one project discovers a pattern, other teams benefit. It's Waze for agent swarms: live routing from real telemetry.

**On screen:** Show a distress alert banner if visible, then briefly show /knowledge route.

---

## [2:50 - 3:00] CLOSE

> Eywa gives humans the steering wheel. Gemini gives them a co-pilot. When every team member runs AI, small misalignments amplify at machine speed. Eywa makes sure you see them before they compound.

**On screen:** Pull back to full HubView. Eywa logo.

---

## Key talking points if asked questions:
- 7 Gemini tools: agent status, thread history, knowledge base, pattern detection, distress signals, destination tracking, global network query
- Gemini calls tools autonomously (function calling), up to 6 rounds per query
- Proactive alerts: Gemini auto-detects distress and pattern issues on page load
- Curvature metric (kappa): positive = converging, negative = stuck, zero = invisible
- Works with any MCP-compatible agent (Claude Code, Cursor, Windsurf, Gemini CLI, custom)
- Context recovery: checkpoint, distress, auto-recover cycle. Worker auto-warns at 30/50/70 tool calls
- Global knowledge network: agents publish anonymized insights, other rooms discover them
- Everything you see was built by agent swarms coordinating through Eywa itself (dogfooding)
EOF

  echo "  voiceover.md updated with current state"
  echo ""
fi

# -----------------------------------------------
# Step 2: Clear old audio and regenerate
# -----------------------------------------------
if [ "$MODE" = "full" ] || [ "$MODE" = "--audio-only" ]; then
  echo "Step 2: Clearing old audio segments..."
  rm -f "$OUTPUT_DIR"/segment_*.mp3
  rm -f "$OUTPUT_DIR"/silence_*.mp3
  rm -f "$OUTPUT_DIR"/concat.txt
  rm -f "$OUTPUT_DIR"/voiceover-full.mp3
  echo "  Cleared."
  echo ""

  # Update the segment text in generate-voiceover.sh to match current voiceover.md
  # Then run it
  echo "Step 3: Generating fresh audio segments..."

  # Update segment 02 with current milestone count
  sed -i.bak "s/at [0-9]* out of [0-9]* milestones/at ${DONE_COUNT} out of ${TOTAL_COUNT} milestones/" "$SCRIPT_DIR/generate-voiceover.sh"
  sed -i.bak "s/over [0-9]* AI agents/over ${AGENT_COUNT} AI agents/" "$SCRIPT_DIR/generate-voiceover.sh"
  rm -f "$SCRIPT_DIR/generate-voiceover.sh.bak"

  if [ -n "${ELEVENLABS_API_KEY:-}" ]; then
    bash "$SCRIPT_DIR/generate-voiceover.sh"
  else
    echo "  ELEVENLABS_API_KEY not set, skipping audio generation."
    echo "  Set it and run: ./generate-voiceover.sh"
  fi
  echo ""
fi

# -----------------------------------------------
# Step 3: Re-record video
# -----------------------------------------------
if [ "$MODE" = "full" ] || [ "$MODE" = "--video-only" ]; then
  echo "Step 4: Recording fresh demo video..."

  # Clear old recordings
  rm -f "$RECORDINGS_DIR"/*.webm
  rm -f "$RECORDINGS_DIR"/eywa-demo.mp4

  # Check if dev server is running
  if ! curl -s -o /dev/null -w "" http://localhost:5173 2>/dev/null; then
    echo "  ERROR: Dev server not running at localhost:5173"
    echo "  Start it with: cd web && npm run dev"
    exit 1
  fi

  cd "$WEB_DIR"
  npx tsx demo/playwright-demo.ts

  # Convert webm to mp4
  WEBM_FILE=$(ls -t "$RECORDINGS_DIR"/*.webm 2>/dev/null | head -1)
  if [ -n "$WEBM_FILE" ]; then
    echo "  Converting to mp4..."
    ffmpeg -y -i "$WEBM_FILE" -c:v libx264 -preset fast -crf 23 "$RECORDINGS_DIR/eywa-demo.mp4" 2>/dev/null
    DURATION=$(ffprobe -v quiet -show_entries format=duration -of csv=p=0 "$RECORDINGS_DIR/eywa-demo.mp4" 2>/dev/null || echo "?")
    echo "  Video: $RECORDINGS_DIR/eywa-demo.mp4 (${DURATION}s)"
  fi
  echo ""
fi

# -----------------------------------------------
# Step 4: Combine video + audio if both exist
# -----------------------------------------------
FINAL_VIDEO="$RECORDINGS_DIR/eywa-demo.mp4"
FINAL_AUDIO="$OUTPUT_DIR/voiceover-full.mp3"
FINAL_OUTPUT="$RECORDINGS_DIR/eywa-demo-final.mp4"

if [ -f "$FINAL_VIDEO" ] && [ -f "$FINAL_AUDIO" ]; then
  echo "Step 5: Combining video + voiceover..."
  ffmpeg -y -i "$FINAL_VIDEO" -i "$FINAL_AUDIO" -c:v copy -c:a aac -shortest "$FINAL_OUTPUT" 2>/dev/null
  DURATION=$(ffprobe -v quiet -show_entries format=duration -of csv=p=0 "$FINAL_OUTPUT" 2>/dev/null || echo "?")
  echo "  Final: $FINAL_OUTPUT (${DURATION}s)"
fi

echo ""
echo "=== Refresh Complete ==="
echo ""
echo "Files:"
[ -f "$SCRIPT_DIR/voiceover.md" ] && echo "  Text:  $SCRIPT_DIR/voiceover.md"
[ -f "$FINAL_AUDIO" ] && echo "  Audio: $FINAL_AUDIO"
[ -f "$FINAL_VIDEO" ] && echo "  Video: $FINAL_VIDEO"
[ -f "$FINAL_OUTPUT" ] && echo "  Final: $FINAL_OUTPUT"
echo ""
echo "Run again anytime the app changes: ./refresh-demo.sh"
echo ""
